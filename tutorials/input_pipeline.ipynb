{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "#img_path = \"/home/Data/CharactersTrimPad28/\"\n",
    "#img_path = \"./s3mnt/ChineseNumbers/\"\n",
    "img_path = \"/home/Data/ChineseNumbers/\"\n",
    "data_root = pathlib.Path(img_path)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 256 \n",
    "IMG_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset.from_tensor_slices\n",
    "all_image_paths = [str(path) for path in list(data_root.glob('*/*'))]\n",
    "\n",
    "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
    "label_to_index = dict((name, index) for index,name in enumerate(label_names))\n",
    "all_image_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
    "                    for path in all_image_paths]\n",
    "\n",
    "image_count = len(all_image_paths)\n",
    "\n",
    "for i in range(3):\n",
    "    print(all_image_paths[i])\n",
    "image_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_str):\n",
    "    image = tf.image.decode_png(image_str, channels=3)\n",
    "    image = tf.image.resize_images(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image_str = tf.read_file(path)\n",
    "    return preprocess_image(image_str)\n",
    "\n",
    "# The tuples are unpacked into the positional arguments of the mapped function\n",
    "def load_and_preprocess_from_path_label(path, label):\n",
    "    return load_and_preprocess_image(path), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "num_class = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.vgg16.VGG16(\n",
    "                input_shape=input_shape,\n",
    "                include_top=False,\n",
    "                weights='imagenet')\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(num_class, activation='softmax')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    global_average_layer,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test iterate time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(dataset):\n",
    "    overall_start = time.time()\n",
    "    n_image = 0\n",
    "    start = time.time()\n",
    "    \n",
    "    history = model.fit(dataset,\n",
    "                        epochs=1)\n",
    "    ''' \n",
    "    for n_batch, (images, labels) in enumerate(dataset):\n",
    "        n_image += int(images.shape[0])\n",
    "        if n_image%100 == 0:\n",
    "            print(\"\\r{} images: {:.2f} s\".format(n_image, time.time()-start), \n",
    "                                                 end='', flush=True)\n",
    "    print()\n",
    "    ''' \n",
    "    end = time.time()\n",
    "    duration = end-start\n",
    "    \n",
    "    print(\"{} images: {} s\".format(image_count, duration))\n",
    "    print(\"{:0.5f} Images/s\".format(image_count/float(duration)))\n",
    "    print(\"Total time: {}s\".format(end-overall_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "#image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.prefetch(buffer_size=1000) # Only on CPU\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "#image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label)\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "# Prefetch must be final Dataset in input pipeline\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=1000)) \n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Map with num_parallel_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. tf.data.experimental.shuffle_and_repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about [tf.data.experimental.shuffle_and_repeat](https://www.tensorflow.org/api_docs/python/tf/data/experimental/shuffle_and_repeat)\n",
    "```\n",
    "tf.data.experimental.shuffle_and_repeat(\n",
    "    buffer_size,\n",
    "    count=None,\n",
    "    seed=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "\n",
    "#path_label_ds = path_label_ds.shuffle(buffer_size=image_count).repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=AUTOTUNE)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. tf.data.experimental.map_and_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about [tf.data.experimental.map_and_batch](https://www.tensorflow.org/api_docs/python/tf/data/experimental/map_and_batch)\n",
    "```\n",
    "tf.data.experimental.map_and_batch(\n",
    "    map_func,\n",
    "    batch_size,\n",
    "    num_parallel_batches=None,\n",
    "    drop_remainder=False,\n",
    "    num_parallel_calls=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.shuffle(buffer_size=image_count)\n",
    "path_label_ds = path_label_ds.repeat(NUM_EPOCHS)\n",
    "\n",
    "image_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. shuffle_and_repeat + map_and_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "image_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about [tf.data.experimental.prefetch_to_device](https://www.tensorflow.org/api_docs/python/tf/data/experimental/prefetch_to_device)\n",
    "```\n",
    "tf.data.experimental.prefetch_to_device(\n",
    "    device,\n",
    "    buffer_size=None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "image_label_ds = path_label_ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "#image_label_ds = image_label_ds.prefetch(buffer_size=AUTOTUNE) # Only on CPU\n",
    "image_label_ds = image_label_ds.apply(\n",
    "    tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", \n",
    "                                            buffer_size=AUTOTUNE)) # Must be final Dataset in input pipeline\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more about [tf.data.Dataset.cache](https://www.tensorflow.org/tutorials/load_data/images#cache)\n",
    "                                       \n",
    "Use tf.data.Dataset.cache to easily cache calculations across epochs. This is especially performant if the dataq fits in memory\n",
    "```\n",
    "ds = image_label_ds.cache()\n",
    "```\n",
    "\n",
    "One disadvantage to using an in memory cache is that the cache must be rebuilt on each run, giving the same startup delay each time the dataset is started:\n",
    "If the data doesn't fit in memory, use a cache file. \n",
    "The cache file also has the advantage that it can be used to quickly restart the dataset without rebuilding the cache. Note how much faster it is the second time:\n",
    "\n",
    "\n",
    "```\n",
    "ds = image_label_ds.cache(filename='./cache.tf-data')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "ds = ds.apply(\n",
    "    tf.data.experimental.map_and_batch(load_and_preprocess_from_path_label, BATCH_SIZE, num_parallel_calls=AUTOTUNE))\n",
    "\n",
    "# Load\n",
    "ds = ds.cache(filename='./cache.tf-ds')\n",
    "#ds = ds.prefetch(buffer_size=AUTOTUNE) # Only on CPU\n",
    "ds = ds.apply(tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=1)) # Must be final Dataset in input pipeline\n",
    "timeit(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "path_label_ds = tf.data.Dataset.from_tensor_slices((all_image_paths, all_image_labels))\n",
    "\n",
    "# Transform\n",
    "path_label_ds = path_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=image_count, count=NUM_EPOCHS))\n",
    "path_label_ds = path_label_ds.cache(filename='./cache.tf-path')\n",
    "\n",
    "image_label_ds = path_label_ds.map(load_and_preprocess_from_path_label, num_parallel_calls=4)\n",
    "image_label_ds = image_label_ds.batch(BATCH_SIZE)\n",
    "\n",
    "# Load\n",
    "image_label_ds = image_label_ds.cache(filename='./cache.tf-image')\n",
    "image_label_ds = image_label_ds.prefetch(buffer_size=10) # Only on CPU\n",
    "#image_label_ds = image_label_ds.apply(tf.data.experimental.prefetch_to_device(device=\"/gpu:0\", buffer_size=1)) # Must be final Dataset in input pipeline\n",
    "timeit(image_label_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
